# Development Environment - Enhanced Resources

Este ambiente est√° optimizado para **desarrollo activo** con recursos balanceados que proporcionan mejor rendimiento que el POC manteniendo costos razonables.

## üí∞ Estimaci√≥n de Costos (Mensual)

### Desglose por Servicio:

| Servicio | Configuraci√≥n | Costo Estimado |
|----------|---------------|----------------|
| **Amazon MSK** | 3 √ó kafka.m5.large + 300GB EBS | ~$180/mes |
| **Kinesis Analytics (Flink)** | 1-2 KPU | ~$110-220/mes |
| **Amazon EKS** | Control plane + 2 √ó t3.medium | ~$125/mes |
| **VPC/Networking** | 2 NAT Gateways + EIPs | ~$70/mes |
| **Load Balancer** | 1 ALB + target groups | ~$25/mes |
| **Storage** | EBS enhanced + logs | ~$40/mes |

### **Total Estimado: ~$580/mes (~$6,960/a√±o)**

> ‚úÖ **Ideal para**: Desarrollo activo, testing de performance, datasets medianos

## üèóÔ∏è Arquitectura Development

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Multi-AZ      ‚îÇ    ‚îÇ      MSK        ‚îÇ    ‚îÇ      EKS        ‚îÇ
‚îÇ   VPC Setup     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   3 Brokers     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   2 Nodes       ‚îÇ
‚îÇ   us-east-1a/b  ‚îÇ    ‚îÇ   m5.large      ‚îÇ    ‚îÇ   t3.medium     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                       ‚îÇ
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
                       ‚îÇ     Flink       ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ   1-2 KPU       ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ     Pinot       ‚îÇ
                       ‚îÇ Controller: 2   ‚îÇ
                       ‚îÇ Broker: 2       ‚îÇ
                       ‚îÇ Server: 2       ‚îÇ
                       ‚îÇ Minion: 1       ‚îÇ
                       ‚îÇ Zookeeper: 3    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Mejoras vs POC

### ‚úÖ **Capacidades Mejoradas:**

| Aspecto | POC | Development | Mejora |
|---------|-----|-------------|--------|
| **Throughput** | ~1K msg/sec | ~10K msg/sec | **10x** |
| **Storage** | ~15GB total | ~300GB total | **20x** |
| **Availability** | Single AZ | Multi-AZ | **HA** |
| **Query Performance** | 1 broker | 2 brokers | **2x** |
| **Data Retention** | 1-7 d√≠as | 30-90 d√≠as | **10x** |
| **Concurrent Users** | 1-2 | 5-10 | **5x** |

### üîß **Configuraciones Clave:**

#### **MSK (Kafka):**
- **Instancias**: 3 √ó kafka.m5.large (vs 2 √ó t3.small)
- **Storage**: 100GB por broker (vs 10GB)
- **Multi-AZ**: Distribuci√≥n en 2 AZs
- **Throughput**: ~10K mensajes/segundo

#### **EKS (Kubernetes):**
- **Nodos**: 2 √ó t3.medium (vs 1 √ó t3.small)
- **Auto-scaling**: 2-4 nodos seg√∫n demanda
- **Storage**: 50GB por nodo (vs 10GB)
- **Multi-AZ**: Nodos distribuidos

#### **Pinot (Analytics):**
- **Controllers**: 2 replicas (HA)
- **Brokers**: 2 replicas (mejor query performance)
- **Servers**: 2 replicas (distribuci√≥n de datos)
- **Zookeeper**: 3 nodos (ensemble confiable)
- **Storage**: 100GB para datos (vs 5GB)

## üöÄ Despliegue Development

### 1. Preparaci√≥n

```bash
cd /Users/daniel.melguizo/Documents/repositories/msk_flink_pinot/terraform/envs/dev

# Inicializar Terraform
terraform init

# Validar configuraci√≥n
terraform validate
```

### 2. Planificaci√≥n

```bash
# Ver qu√© recursos se van a crear
terraform plan

# Guardar el plan para revisi√≥n
terraform plan -out=dev.tfplan
```

### 3. Despliegue

```bash
# Aplicar la configuraci√≥n
terraform apply dev.tfplan

# Tiempo estimado de despliegue: 15-20 minutos
```

### 4. Verificaci√≥n Post-Despliegue

```bash
# Ver outputs importantes
terraform output

# Configurar kubectl
aws eks update-kubeconfig --region us-east-1 --name dev-pinot-cluster

# Verificar nodos EKS
kubectl get nodes

# Verificar pods de Pinot
kubectl get pods -n pinot-dev

# Verificar MSK cluster
aws kafka describe-cluster --cluster-arn $(terraform output -raw kafka_cluster_arn)
```

## üîß Configuraci√≥n Post-Despliegue

### Acceder a Pinot Controller

```bash
# Port forward para acceder a Pinot Controller
kubectl port-forward -n pinot-dev svc/pinot-controller 9000:9000

# Abrir en navegador: http://localhost:9000
```

### Configurar Kafka Topics

```bash
# Obtener bootstrap servers
BOOTSTRAP_SERVERS=$(terraform output -raw kafka_bootstrap_brokers)

# Crear topics para desarrollo
kafka-topics.sh --create --topic events --bootstrap-server $BOOTSTRAP_SERVERS \
  --partitions 6 --replication-factor 3

kafka-topics.sh --create --topic metrics --bootstrap-server $BOOTSTRAP_SERVERS \
  --partitions 3 --replication-factor 3
```

### Configurar Pinot Tables

```bash
# Ejemplo de tabla para eventos
cat > events_table.json << EOF
{
  "tableName": "events",
  "tableType": "REALTIME",
  "segmentsConfig": {
    "timeColumnName": "timestamp",
    "timeType": "MILLISECONDS",
    "retentionTimeUnit": "DAYS",
    "retentionTimeValue": "30"
  },
  "tenants": {},
  "tableIndexConfig": {
    "loadMode": "MMAP",
    "streamConfigs": {
      "streamType": "kafka",
      "stream.kafka.consumer.type": "lowlevel",
      "stream.kafka.topic.name": "events",
      "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder",
      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
      "stream.kafka.broker.list": "$BOOTSTRAP_SERVERS",
      "realtime.segment.flush.threshold.rows": "1000000",
      "realtime.segment.flush.threshold.time": "3600000"
    }
  },
  "metadata": {
    "customConfigs": {}
  }
}
EOF

# Crear tabla en Pinot
curl -X POST "http://localhost:9000/tables" \
  -H "Content-Type: application/json" \
  -d @events_table.json
```

## üìä Monitoreo y Observabilidad

### CloudWatch Dashboards

```bash
# Crear dashboard personalizado para development
aws cloudwatch put-dashboard --dashboard-name "MSK-Flink-Pinot-Dev" --dashboard-body '{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "metrics": [
          ["AWS/MSK", "BytesInPerSec", "Cluster Name", "dev-msk-cluster"],
          [".", "BytesOutPerSec", ".", "."]
        ],
        "period": 300,
        "stat": "Average",
        "region": "us-east-1",
        "title": "MSK Throughput"
      }
    }
  ]
}'
```

### Logs Centralizados

```bash
# Configurar log forwarding desde EKS
kubectl apply -f - << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: amazon-cloudwatch
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020

    [INPUT]
        Name              tail
        Tag               pinot.*
        Path              /var/log/containers/pinot*.log
        Parser            docker
        DB                /var/log/flb_pinot.db
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
        Refresh_Interval  10

    [OUTPUT]
        Name                cloudwatch_logs
        Match               pinot.*
        region              us-east-1
        log_group_name      /aws/eks/dev-pinot-cluster/pinot
        log_stream_prefix   dev-
        auto_create_group   true
EOF
```

## üîÑ Workflows de Desarrollo

### 1. **Desarrollo de Schemas**

```bash
# Directorio para schemas
mkdir -p schemas/

# Ejemplo de schema para eventos
cat > schemas/events_schema.json << EOF
{
  "schemaName": "events",
  "dimensionFieldSpecs": [
    {"name": "userId", "dataType": "STRING"},
    {"name": "eventType", "dataType": "STRING"},
    {"name": "platform", "dataType": "STRING"}
  ],
  "metricFieldSpecs": [
    {"name": "duration", "dataType": "LONG"},
    {"name": "count", "dataType": "INT"}
  ],
  "dateTimeFieldSpecs": [
    {"name": "timestamp", "dataType": "LONG", "format": "1:MILLISECONDS:EPOCH", "granularity": "1:MILLISECONDS"}
  ]
}
EOF
```

### 2. **Testing de Performance**

```bash
# Script de carga de datos de prueba
cat > load_test_data.py << EOF
import json
import time
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers=['$BOOTSTRAP_SERVERS'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Generar datos de prueba
for i in range(10000):
    event = {
        'userId': f'user_{i % 1000}',
        'eventType': 'click',
        'platform': 'web',
        'timestamp': int(time.time() * 1000),
        'duration': 150 + (i % 300),
        'count': 1
    }
    producer.send('events', event)
    
    if i % 1000 == 0:
        print(f'Sent {i} events')

producer.flush()
print('Load test completed')
EOF

python3 load_test_data.py
```

### 3. **Queries de Desarrollo**

```sql
-- Queries t√≠picas para desarrollo
SELECT 
    eventType,
    COUNT(*) as event_count,
    AVG(duration) as avg_duration
FROM events 
WHERE timestamp > ago('1h')
GROUP BY eventType
ORDER BY event_count DESC;

-- Query de performance con filtros
SELECT 
    platform,
    eventType,
    COUNT(*) as events,
    PERCENTILE_TDIGEST(duration, 95) as p95_duration
FROM events 
WHERE timestamp BETWEEN ago('24h') AND now()
  AND userId IN ('user_1', 'user_2', 'user_3')
GROUP BY platform, eventType;
```

## üí° Optimizaciones de Desarrollo

### 1. **Spot Instances para Ahorro**

```hcl
# En main.tf, agregar para nodos EKS:
capacity_type = "SPOT"
# Ahorro potencial: 50-70%
```

### 2. **Scheduled Scaling**

```bash
# Script para escalar durante horarios de desarrollo
cat > scale_dev_cluster.sh << EOF
#!/bin/bash
HOUR=$(date +%H)

if [ $HOUR -ge 9 ] && [ $HOUR -le 18 ]; then
    # Horario laboral: escalar a 2-4 nodos
    aws eks update-nodegroup-config \
        --cluster-name dev-pinot-cluster \
        --nodegroup-name dev-nodes \
        --scaling-config minSize=2,maxSize=4,desiredSize=2
else
    # Fuera de horario: escalar a 1 nodo
    aws eks update-nodegroup-config \
        --cluster-name dev-pinot-cluster \
        --nodegroup-name dev-nodes \
        --scaling-config minSize=1,maxSize=2,desiredSize=1
fi
EOF

# Programar con cron
# 0 9,18 * * 1-5 /path/to/scale_dev_cluster.sh
```

### 3. **Backup Autom√°tico**

```bash
# Script de backup para desarrollo
cat > backup_dev_data.sh << EOF
#!/bin/bash
DATE=$(date +%Y%m%d)
S3_BUCKET="your-dev-backups-bucket"

# Backup de configuraciones Pinot
kubectl get configmaps -n pinot-dev -o yaml > pinot-configs-$DATE.yaml
aws s3 cp pinot-configs-$DATE.yaml s3://$S3_BUCKET/configs/

# Backup de schemas
curl -X GET "http://localhost:9000/schemas" > schemas-$DATE.json
aws s3 cp schemas-$DATE.json s3://$S3_BUCKET/schemas/

echo "Backup completed for $DATE"
EOF
```

## üö® Limitaciones del Ambiente Development

### ‚ö†Ô∏è **Consideraciones**:

- **No es para producci√≥n**: Configuraci√≥n optimizada para desarrollo
- **Datos temporales**: Retenci√≥n de 30-90 d√≠as
- **Monitoreo b√°sico**: CloudWatch est√°ndar
- **Backup manual**: No hay backup autom√°tico configurado
- **Seguridad b√°sica**: Security groups y IAM b√°sicos

## üîÑ Upgrade Path a Staging/Producci√≥n

### Para escalar a staging:

1. **Aumentar instancias**:
   ```hcl
   kafka_instance_type = "kafka.m5.xlarge"
   eks_node_instance_types = ["t3.large"]
   ```

2. **Multi-regi√≥n**:
   ```hcl
   # Configurar cross-region replication
   ```

3. **Enhanced monitoring**:
   - Prometheus + Grafana
   - Custom metrics
   - Alerting rules

4. **Backup autom√°tico**:
   - S3 lifecycle policies
   - Cross-region backup
   - Point-in-time recovery

## üßπ Cleanup

```bash
# Destruir ambiente development
terraform destroy

# Confirmar destrucci√≥n
# IMPORTANTE: Esto eliminar√° TODOS los recursos y datos
```

---

**üéØ Objetivo Development**: Ambiente robusto para desarrollo activo con balance entre performance y costo.

**üí∞ Costo por Duraci√≥n:**

| Duraci√≥n | Costo Total | Ideal Para |
|----------|-------------|------------|
| **8 horas** | **~$3.50** | D√≠a de desarrollo |
| **40 horas** | **~$17.50** | Semana de desarrollo |
| **160 horas** | **~$70** | Sprint mensual |
| **1 mes** | **~$580** | Desarrollo continuo |
